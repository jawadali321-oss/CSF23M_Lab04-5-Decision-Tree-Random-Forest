- `exercise-random-forests.ipynb`
 Lab 4: Decision Tree
6. Random Forests
 Labs Completed
 Submission Requirements
| LinkedIn Profile | www.linkedin.com/in/jawad-ali-43b56a247 |
- Building Random Forest models for improved accuracy
- Competition: Housing Prices Competition for Kaggle Learn Users
- Python 3.x
- pandas
- Validating model performance using MAE (Mean Absolute Error)
---
- Building and training machine learning models
|---|---|
Course Link: https://www.kaggle.com/learn/intro-to-machine-learning  
---
 Competition Result
 Technologies Used
4. Model Validation
 Machine Learning Labs 4 & 5 – Decision Tree & Random Forest
- Understanding underfitting vs overfitting
|---|---|
| Kaggle Certificate | https://www.kaggle.com/learn/certification/jawadali999/intro-to-machine-learning |
| Roll Number | BCSF23M541 |
 Deadline
1. How Models Work
2. Basic Data Exploration
- `exercise-machine-learning-competitions.ipynb`
 Key Skills Learned
- Status: ✅ Successfully Submitted
---
- `exercise-underfitting-and-overfitting.ipynb`
7. Machine Learning Competitions
 Repository Contents
| Name | Jawad Ali |
 Lab 5: Random Forest
| Field | Details |
---
| Requirement | Link |
---
3. Your First Machine Learning Model
 Overview
---
---
| Time Invested | ~4 hours |
5. Underfitting and Overfitting
 Student Information
22 Feb, 2026 (Sunday) by 5 PM
- `exercise-explore-your-data.ipynb`
- Tuning `max_leaf_nodes` for optimal tree size
| Completion Date | February 2026 |
- Understanding how decision tree models work
- `exercise-model-validation.ipynb`
This repository contains completed exercises from the Kaggle Intro to Machine Learning Mini Course as part of Machine Learning Labs 04 & 05.
- Submitting predictions to a Kaggle competition
Estimated Time to Complete: 3 to 4 hours
- `exercise-your-first-machine-learning-model.ipynb`
- scikit-learn
- Public Score (MAE): 21,217.916
| GitHub Repository | https://github.com/jawadali321-oss/CSF23M_Lab04-5-Decision-Tree-Random-Forest |
- Loading and exploring data with pandas
 numpy
